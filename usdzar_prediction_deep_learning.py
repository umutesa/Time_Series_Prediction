# -*- coding: utf-8 -*-
"""USDZAR_Prediction_Deep_learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EOvrHVe2YTzjfOK4nEfjUxV6hbTQXdmg
"""

pip install --upgrade yfinance

"""
This program forecasts the USD/ZAR currency exchange rate using data from the Yahoo database.
It evaluates four models—ANN, RNN, LSTM, and GRU—comparing their performance.
The results are analyzed and discussed based on the efficiency of each model.
"""
import yfinance as yf #import the libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import GRU

import keras.backend as K
from sklearn import metrics
from sklearn.metrics import r2_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

start_date = '2010-01-01'  #extract the data from the yfinace
end_date = '2025-05-05'
df = yf.Ticker('USDZAR=X').history(start=start_date, end=end_date)
#df = yf.download('USDZAR=X', start=start_date, end=end_date, auto_adjust=False, progress=False, threads=False)
#df = yf.download('USDZAR=X', start=start_date, end=end_date,  auto_adjust=False)

print(df)  #display the data
plt.figure(figsize=(12, 6)) #plot the data
plt.plot(df.index, df['Close'], label='1 USD TO ZAR/RAND', color='b')
plt.title('USDZAR Currency Data')
plt.xlabel('Year')
plt.ylabel('ZAR/RAND')
plt.grid(True)
plt.legend()
plt.show()

print('Statistics Data----------------------------------------------------------------')
print(df.describe())  #display the data

data = df[['Close']]  #Use the "Close" price
scaler = MinMaxScaler(feature_range=(0, 1)) #scales the data using 0 as min and 1 as max
scaled_data = scaler.fit_transform(data)
scaled_df = pd.DataFrame(scaled_data, columns=['Close'])
print(scaled_df)

"""
Divide the dataset using an 80%-20% split to maintain a balanced model, reducing
the risks of overfitting and underfitting. The 80% portion is used for training,
ensuring the model learns effectively, while the 20% portion is reserved for testing,
validating performance on unseen data.
"""
#split_date = pd.Timestamp('01-01-2020')
split_index = int(len(scaled_df) * 0.8)

# Split the dataset
train = scaled_df.iloc[:split_index]
test = scaled_df.iloc[split_index:]

fig, ax = plt.subplots(figsize=(12, 8)) #plot the data
ax.plot(train.index, train['Close'], linestyle='-', alpha=0.7, linewidth=2, label='Train', color='blue')
ax.plot(test.index, test['Close'], linestyle='--', alpha=0.7, linewidth=2, label='Test', color='red')
ax.set_title('Train vs Test Data', fontsize=14)
ax.set_xlabel('Year', fontsize=12)
ax.set_ylabel('ZAR/RAND', fontsize=12)
ax.legend()
ax.grid(True)
plt.show()

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
train_sc = sc.fit_transform(train)
test_sc = sc.transform(test)
X_train = train_sc[:-1]
y_train = train_sc[1:]
X_test = test_sc[:-1]
y_test = test_sc[1:]

print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")

"""
Defining the LSTM Function:
A type of RNN that solves the vanishing gradient issue using memory cells and gates (input, forget, and output gates).
LSTMs are highly effective for long-term dependencies.
"""

def LSTM_model(X_train):
    # Ensure X_train is a NumPy array
    X_train = np.array(X_train)

    # Reshape for LSTM input: (samples, time steps, features)
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

    model = Sequential()
    model.add(Input(shape=(X_train.shape[1], 1)))
    model.add(LSTM(units=50, return_sequences=True))
    model.add(Dropout(0.2))

    model.add(LSTM(units=50, return_sequences=False))
    model.add(Dropout(0.2))

    model.add(Dense(units=25))
    model.add(Dense(units=1))

    model.compile(optimizer='adam', loss='mean_squared_error')
    model.summary()

    return model

"""
Defining the RNN Function :  A neural network designed for sequence-based tasks like time series forecasting or text processing.
Unlike ANNs, RNNs retain previous inputs through loops, making them suitable for sequential data.
 However, they suffer from vanishing gradient problems, limiting long-term memory.
"""

def RNN_model():
  model = Sequential()
  model.add(Input(shape=(X_train.shape[1], 1)))
  model.add(LSTM(50, return_sequences=True))
  model.add(Dense(units=1))
  model.compile(optimizer='adam', loss='mean_squared_error')
  model.summary()
  return model

"""
 Defining the ANN Function: The most basic neural network architecture.
 It consists of layers of neurons: input, hidden, and output layers.
 ANNs are powerful for pattern recognition but struggle with sequential data.
"""

def ANN():
  K.clear_session()
  model = Sequential()
  model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))
  model.add(Dense(1))
  model.summary()
  return model

"""
Defining the GRU Function :  A simplified version of LSTM with fewer parameters, resulting in faster training and execution.
Instead of separate input, output, and forget gates, GRUs use reset and update gates, making them computationally efficient
while still capturing dependencies
"""

def GRU_model():
  K.clear_session()
  model = Sequential()
  model.add(GRU(7, input_shape=(1, X_train.shape[1]), activation='linear', kernel_initializer='lecun_uniform', return_sequences=False))
  model.add(Dense(1))
  model.summary()
  return model

# train the ANN model
model = ANN()
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=120, batch_size=32)

#predict the model
y_pred_test = model.predict(X_test)
y_train_pred = model.predict(X_train)

#performance metrices
train_mse = metrics.mean_squared_error(y_train, y_train_pred)
rmse_train = np.sqrt(train_mse)
test_mse = metrics.mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(test_mse)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_pred_test)

print("The R2 score on the Train set is:\t{:0.3f}".format(r2_score(y_train, y_train_pred)))
print("The  MAE on the Train set is:\t{:0.3f}".format(metrics.mean_absolute_error(y_train, y_train_pred)))
print("The  RMSE on the Train set is:\t{:0.3f}".format(rmse_train))
print("The R2 score on the Test set is:\t{:0.3f}".format(r2_score(y_test, y_pred_test)))
print("The  MAE on the Test set is:\t{:0.3f}".format(metrics.mean_absolute_error(y_test, y_pred_test)))
print("The  RMSE on the Test set is:\t{:0.3f}".format(rmse_test))

# Define the data
metrics_names = ["R2 Score", "MAE", "RMSE"]
train_values = [r2_score(y_train, y_train_pred), metrics.mean_absolute_error(y_train, y_train_pred), rmse_train]
test_values = [r2_score(y_test, y_pred_test), metrics.mean_absolute_error(y_test, y_pred_test), rmse_test]
df = pd.DataFrame({  #create a dataframe
    "Metric": metrics_names * 2,
    "Value": train_values + test_values,
    "Dataset": ["Train"] * 3 + ["Test"] * 3
})

plt.figure(figsize=(10, 6))
colors = ['#1f77b4' if dataset == 'Train' else '#ff7f0e' for dataset in df["Dataset"]]
bars = plt.barh(df["Metric"] + " (" + df["Dataset"] + ")", df["Value"], color=colors)
for bar in bars:
    plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
             f'{bar.get_width():.3f}', va='center')

plt.title("Model Evaluation Metrics: Train vs Test", fontsize=16, fontweight='bold')
plt.xlabel("Metric Value", fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Metrics values
metrics_labels = ["R2 Score", "MAE", "RMSE"]
train_values = [r2_train, metrics.mean_absolute_error(y_train, y_train_pred), rmse_train]
test_values = [r2_test, metrics.mean_absolute_error(y_test, y_pred_test), rmse_test]

x = np.arange(len(metrics_labels))  # Label positions

plt.figure(figsize=(8, 6))
plt.plot(metrics_labels, train_values, marker="o", linestyle="-", label="Train Set", color="blue")
plt.plot(metrics_labels, test_values, marker="o", linestyle="--", label="Test Set", color="red")

plt.ylabel("Value")
plt.title("ANN Model Performance Metrics Trend")
plt.legend()
plt.show()

labels = np.array(["R2 Score", "MAE", "RMSE"])
angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()
angles += angles[:1]  # Closing the loop

def plot_radar(metrics, title, color):
    values = np.array(list(metrics.values()))
    stats = np.append(values, values[0])  # Closing the radar shape

    plt.figure(figsize=(7, 7))
    ax = plt.subplot(111, polar=True)

    # Styling
    ax.set_facecolor("#f0f0f0")
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)

    # Plotting radar chart
    ax.plot(angles, stats, linewidth=2, linestyle='solid', label=title, color=color)
    ax.fill(angles, stats, alpha=0.3, color=color)

    # Labels and details
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(labels, fontsize=8, fontweight="bold")
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])  # Grid lines
    ax.set_yticklabels(["0.2", "0.4", "0.6", "0.8", "1.0"], fontsize=7)

    # Annotate points with actual values
    for i, txt in enumerate(stats[:-1]):
        ax.text(angles[i], txt + 0.05, f"{txt:.2f}", color=color, fontsize=8, ha='center')

    #plt.title(title, fontsize=7, fontweight="bold")
    plt.legend()
    plt.show()

metrics_train = {"R2 Score": r2_train, "MAE": metrics.mean_absolute_error(y_train, y_train_pred), "RMSE": rmse_train}
metrics_test = {"R2 Score": r2_test, "MAE": metrics.mean_absolute_error(y_test, y_pred_test), "RMSE": rmse_test}

plot_radar(metrics_test, "ANN Performance", "red")

plt.figure(figsize=(12, 6))
plt.plot(y_test, label='True Values', linewidth=2, color='royalblue')
plt.plot(y_pred_test, label='ANN Predictions', linewidth=2, linestyle='--', color='tomato')
plt.title("ANN's Prediction vs True Values", fontsize=16, fontweight='bold')
plt.xlabel('Observation Index', fontsize=14)
plt.ylabel('INR (Scaled)', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(frameon=True, shadow=True, fontsize=12)
plt.tight_layout()

# train the RNN model
model = RNN_model()
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=120, batch_size=32)

#predict the model
y_pred_test = model.predict(X_test)
y_train_pred = model.predict(X_train)

y_train_pred = y_train_pred.reshape(-1)
y_train = y_train.reshape(-1)

y_pred_test = y_pred_test.reshape(-1)
y_test = y_test.reshape(-1)

#performance metrices
train_mse = metrics.mean_squared_error(y_train, y_train_pred)
rmse_train = np.sqrt(train_mse)
test_mse = metrics.mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(test_mse)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_pred_test)

print("The R2 score on the Train set is:\t{:0.3f}".format(r2_score(y_train, y_train_pred)))
print("The  MAE on the Train set is:\t{:0.3f}".format(metrics.mean_absolute_error(y_train, y_train_pred)))
print("The  RMSE on the Train set is:\t{:0.3f}".format(rmse_train))
print("The R2 score on the Test set is:\t{:0.3f}".format(r2_score(y_test, y_pred_test)))
print("The  MAE on the Test set is:\t{:0.3f}".format(metrics.mean_absolute_error(y_test, y_pred_test)))
print("The  RMSE on the Test set is:\t{:0.3f}".format(rmse_test))

# Define the data
metrics_names = ["R2 Score", "MAE", "RMSE"]
train_values = [r2_score(y_train, y_train_pred), metrics.mean_absolute_error(y_train, y_train_pred), rmse_train]
test_values = [r2_score(y_test, y_pred_test), metrics.mean_absolute_error(y_test, y_pred_test), rmse_test]
df = pd.DataFrame({  #create a dataframe
    "Metric": metrics_names * 2,
    "Value": train_values + test_values,
    "Dataset": ["Train"] * 3 + ["Test"] * 3
})

plt.figure(figsize=(10, 6))
colors = ['#1f77b4' if dataset == 'Train' else '#ff7f0e' for dataset in df["Dataset"]]
bars = plt.barh(df["Metric"] + " (" + df["Dataset"] + ")", df["Value"], color=colors)
for bar in bars:
    plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
             f'{bar.get_width():.3f}', va='center')

plt.title("Model Evaluation Metrics: Train vs Test", fontsize=16, fontweight='bold')
plt.xlabel("Metric Value", fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Metrics values
metrics_labels = ["R2 Score", "MAE", "RMSE"]
train_values = [r2_train, metrics.mean_absolute_error(y_train, y_train_pred), rmse_train]
test_values = [r2_test, metrics.mean_absolute_error(y_test, y_pred_test), rmse_test]

x = np.arange(len(metrics_labels))  # Label positions

plt.figure(figsize=(8, 6))
plt.plot(metrics_labels, train_values, marker="o", linestyle="-", label="Train Set", color="blue")
plt.plot(metrics_labels, test_values, marker="o", linestyle="--", label="Test Set", color="red")

plt.ylabel("Value")
plt.title("RNN Model Performance Metrics Trend")
plt.legend()
plt.show()

labels = np.array(["R2 Score", "MAE", "RMSE"])
angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()
angles += angles[:1]  # Closing the loop

def plot_radar(metrics, title, color):
    values = np.array(list(metrics.values()))
    stats = np.append(values, values[0])  # Closing the radar shape

    plt.figure(figsize=(7, 7))
    ax = plt.subplot(111, polar=True)

    # Styling
    ax.set_facecolor("#f0f0f0")
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)

    # Plotting radar chart
    ax.plot(angles, stats, linewidth=2, linestyle='solid', label=title, color=color)
    ax.fill(angles, stats, alpha=0.3, color=color)

    # Labels and details
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(labels, fontsize=8, fontweight="bold")
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])  # Grid lines
    ax.set_yticklabels(["0.2", "0.4", "0.6", "0.8", "1.0"], fontsize=7)

    # Annotate points with actual values
    for i, txt in enumerate(stats[:-1]):
        ax.text(angles[i], txt + 0.05, f"{txt:.2f}", color=color, fontsize=8, ha='center')

    #plt.title(title, fontsize=7, fontweight="bold")
    plt.legend()
    plt.show()

metrics_train = {"R2 Score": r2_train, "MAE": metrics.mean_absolute_error(y_train, y_train_pred), "RMSE": rmse_train}
metrics_test = {"R2 Score": r2_test, "MAE": metrics.mean_absolute_error(y_test, y_pred_test), "RMSE": rmse_test}

plot_radar(metrics_test, "RNN Performance", "red")

plt.figure(figsize=(12, 6))
plt.plot(y_test, label='True Values', linewidth=2, color='royalblue')
plt.plot(y_pred_test, label='RNN Predictions', linewidth=2, linestyle='--', color='tomato')
plt.title("RNN's Prediction vs True Values", fontsize=16, fontweight='bold')
plt.xlabel('Observation Index', fontsize=14)
plt.ylabel('INR (Scaled)', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(frameon=True, shadow=True, fontsize=12)
plt.tight_layout()

# train the LSTM model
model = LSTM_model(X_train)
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=120, batch_size=32)

#predict the model
y_pred_test = model.predict(X_test)
y_train_pred = model.predict(X_train)

#performance metrices
train_mse = metrics.mean_squared_error(y_train, y_train_pred)
rmse_train = np.sqrt(train_mse)
test_mse = metrics.mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(test_mse)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_pred_test)

print("The R2 score on the Train set is:\t{:0.3f}".format(r2_score(y_train, y_train_pred)))
print("The  MAE on the Train set is:\t{:0.3f}".format(metrics.mean_absolute_error(y_train, y_train_pred)))
print("The  RMSE on the Train set is:\t{:0.3f}".format(rmse_train))
print("The R2 score on the Test set is:\t{:0.3f}".format(r2_score(y_test, y_pred_test)))
print("The  MAE on the Test set is:\t{:0.3f}".format(metrics.mean_absolute_error(y_test, y_pred_test)))
print("The  RMSE on the Test set is:\t{:0.3f}".format(rmse_test))

# Define the data
metrics_names = ["R2 Score", "MAE", "RMSE"]
train_values = [r2_score(y_train, y_train_pred), metrics.mean_absolute_error(y_train, y_train_pred), rmse_train]
test_values = [r2_score(y_test, y_pred_test), metrics.mean_absolute_error(y_test, y_pred_test), rmse_test]
df = pd.DataFrame({  #create a dataframe
    "Metric": metrics_names * 2,
    "Value": train_values + test_values,
    "Dataset": ["Train"] * 3 + ["Test"] * 3
})

plt.figure(figsize=(10, 6))
colors = ['#1f77b4' if dataset == 'Train' else '#ff7f0e' for dataset in df["Dataset"]]
bars = plt.barh(df["Metric"] + " (" + df["Dataset"] + ")", df["Value"], color=colors)
for bar in bars:
    plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
             f'{bar.get_width():.3f}', va='center')

plt.title("Model Evaluation Metrics: Train vs Test", fontsize=16, fontweight='bold')
plt.xlabel("Metric Value", fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Metrics values
metrics_labels = ["R2 Score", "MAE", "RMSE"]
train_values = [r2_train, metrics.mean_absolute_error(y_train, y_train_pred), rmse_train]
test_values = [r2_test, metrics.mean_absolute_error(y_test, y_pred_test), rmse_test]

x = np.arange(len(metrics_labels))  # Label positions

plt.figure(figsize=(8, 6))
plt.plot(metrics_labels, train_values, marker="o", linestyle="-", label="Train Set", color="blue")
plt.plot(metrics_labels, test_values, marker="o", linestyle="--", label="Test Set", color="red")

plt.ylabel("Value")
plt.title("LSTM Model Performance Metrics Trend")
plt.legend()
plt.show()

labels = np.array(["R2 Score", "MAE", "RMSE"])
angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()
angles += angles[:1]  # Closing the loop

def plot_radar(metrics, title, color):
    values = np.array(list(metrics.values()))
    stats = np.append(values, values[0])  # Closing the radar shape

    plt.figure(figsize=(7, 7))
    ax = plt.subplot(111, polar=True)

    # Styling
    ax.set_facecolor("#f0f0f0")
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)

    # Plotting radar chart
    ax.plot(angles, stats, linewidth=2, linestyle='solid', label=title, color=color)
    ax.fill(angles, stats, alpha=0.3, color=color)

    # Labels and details
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(labels, fontsize=8, fontweight="bold")
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])  # Grid lines
    ax.set_yticklabels(["0.2", "0.4", "0.6", "0.8", "1.0"], fontsize=7)

    # Annotate points with actual values
    for i, txt in enumerate(stats[:-1]):
        ax.text(angles[i], txt + 0.05, f"{txt:.2f}", color=color, fontsize=8, ha='center')

    #plt.title(title, fontsize=7, fontweight="bold")
    plt.legend()
    plt.show()

metrics_train = {"R2 Score": r2_train, "MAE": metrics.mean_absolute_error(y_train, y_train_pred), "RMSE": rmse_train}
metrics_test = {"R2 Score": r2_test, "MAE": metrics.mean_absolute_error(y_test, y_pred_test), "RMSE": rmse_test}

plot_radar(metrics_test, "LSTM Performance", "red")

plt.figure(figsize=(12, 6))
plt.plot(y_test, label='True Values', linewidth=2, color='royalblue')
plt.plot(y_pred_test, label='LSTM Predictions', linewidth=2, linestyle='--', color='tomato')
plt.title("LSTM's Prediction vs True Values", fontsize=16, fontweight='bold')
plt.xlabel('Observation Index', fontsize=14)
plt.ylabel('INR (Scaled)', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(frameon=True, shadow=True, fontsize=12)
plt.tight_layout()

# train the GRU model
model = GRU_model()
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=120, batch_size=32)

#predict the model
y_pred_test = model.predict(X_test)
y_train_pred = model.predict(X_train)

#performance metrices
train_mse = metrics.mean_squared_error(y_train, y_train_pred)
rmse_train = np.sqrt(train_mse)
test_mse = metrics.mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(test_mse)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_pred_test)

print("The R2 score on the Train set is:\t{:0.3f}".format(r2_score(y_train, y_train_pred)))
print("The  MAE on the Train set is:\t{:0.3f}".format(metrics.mean_absolute_error(y_train, y_train_pred)))
print("The  RMSE on the Train set is:\t{:0.3f}".format(rmse_train))
print("The R2 score on the Test set is:\t{:0.3f}".format(r2_score(y_test, y_pred_test)))
print("The  MAE on the Test set is:\t{:0.3f}".format(metrics.mean_absolute_error(y_test, y_pred_test)))
print("The  RMSE on the Test set is:\t{:0.3f}".format(rmse_test))

# Define the data
metrics_names = ["R2 Score", "MAE", "RMSE"]
train_values = [r2_score(y_train, y_train_pred), metrics.mean_absolute_error(y_train, y_train_pred), rmse_train]
test_values = [r2_score(y_test, y_pred_test), metrics.mean_absolute_error(y_test, y_pred_test), rmse_test]
df = pd.DataFrame({  #create a dataframe
    "Metric": metrics_names * 2,
    "Value": train_values + test_values,
    "Dataset": ["Train"] * 3 + ["Test"] * 3
})

plt.figure(figsize=(10, 6))
colors = ['#1f77b4' if dataset == 'Train' else '#ff7f0e' for dataset in df["Dataset"]]
bars = plt.barh(df["Metric"] + " (" + df["Dataset"] + ")", df["Value"], color=colors)
for bar in bars:
    plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
             f'{bar.get_width():.3f}', va='center')

plt.title("Model Evaluation Metrics: Train vs Test", fontsize=16, fontweight='bold')
plt.xlabel("Metric Value", fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Metrics values
metrics_labels = ["R2 Score", "MAE", "RMSE"]
train_values = [r2_train, metrics.mean_absolute_error(y_train, y_train_pred), rmse_train]
test_values = [r2_test, metrics.mean_absolute_error(y_test, y_pred_test), rmse_test]

x = np.arange(len(metrics_labels))  # Label positions

plt.figure(figsize=(8, 6))
plt.plot(metrics_labels, train_values, marker="o", linestyle="-", label="Train Set", color="blue")
plt.plot(metrics_labels, test_values, marker="o", linestyle="--", label="Test Set", color="red")

plt.ylabel("Value")
plt.title("GRU Model Performance Metrics Trend")
plt.legend()
plt.show()

labels = np.array(["R2 Score", "MAE", "RMSE"])
angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()
angles += angles[:1]  # Closing the loop

def plot_radar(metrics, title, color):
    values = np.array(list(metrics.values()))
    stats = np.append(values, values[0])  # Closing the radar shape

    plt.figure(figsize=(7, 7))
    ax = plt.subplot(111, polar=True)

    # Styling
    ax.set_facecolor("#f0f0f0")
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)

    # Plotting radar chart
    ax.plot(angles, stats, linewidth=2, linestyle='solid', label=title, color=color)
    ax.fill(angles, stats, alpha=0.3, color=color)

    # Labels and details
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(labels, fontsize=8, fontweight="bold")
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])  # Grid lines
    ax.set_yticklabels(["0.2", "0.4", "0.6", "0.8", "1.0"], fontsize=7)

    # Annotate points with actual values
    for i, txt in enumerate(stats[:-1]):
        ax.text(angles[i], txt + 0.05, f"{txt:.2f}", color=color, fontsize=8, ha='center')

    #plt.title(title, fontsize=7, fontweight="bold")
    plt.legend()
    plt.show()

metrics_train = {"R2 Score": r2_train, "MAE": metrics.mean_absolute_error(y_train, y_train_pred), "RMSE": rmse_train}
metrics_test = {"R2 Score": r2_test, "MAE": metrics.mean_absolute_error(y_test, y_pred_test), "RMSE": rmse_test}

plot_radar(metrics_test, "GRU Performance", "red")

plt.figure(figsize=(12, 6))
plt.plot(y_test, label='True Values', linewidth=2, color='royalblue')
plt.plot(y_pred_test, label='GRU Predictions', linewidth=2, linestyle='--', color='tomato')
plt.title("GRU's Prediction vs True Values", fontsize=16, fontweight='bold')
plt.xlabel('Observation Index', fontsize=14)
plt.ylabel('INR (Scaled)', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(frameon=True, shadow=True, fontsize=12)
plt.tight_layout()